import { litellmService } from './litellm.service';

export interface SemanticChunk {
    index: number;
    title: string;
    text: string;
}

const SEMANTIC_CHUNKING_PROMPT = `–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ä–∞–∑–±–∏–µ–Ω–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —á–∞—Å—Ç–∏ –¥–ª—è RAG-—Å–∏—Å—Ç–µ–º.

–ó–ê–î–ê–ß–ê: –†–∞–∑–±–µ–π —Ç–µ–∫—Å—Ç –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏. –ö–∞–∂–¥—ã–π –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–π –º—ã—Å–ª—å—é –∏–ª–∏ —Ç–µ–º–æ–π.

–ü–†–ê–í–ò–õ–ê:
1. –ö–∞–∂–¥—ã–π —á–∞–Ω–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—É—é –º—ã—Å–ª—å/—Ç–µ–º—É
2. –ù–µ —Ä–∞–∑—Ä—ã–≤–∞–π –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, —Å–ø–∏—Å–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
3. –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: 500-2000 —Å–∏–º–≤–æ–ª–æ–≤
4. –°–æ—Ö—Ä–∞–Ω—è–π –Ω—É–º–µ—Ä–∞—Ü–∏—é –ø—É–Ω–∫—Ç–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
5. –î–∞–≤–∞–π –∫–æ—Ä–æ—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–∏–π JSON):
{
  "chunks": [
    {"index": 0, "title": "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã", "text": "–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞..."},
    {"index": 1, "title": "–°–ª–µ–¥—É—é—â–∞—è —Ç–µ–º–∞", "text": "..."}
  ]
}

–í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON, –±–µ–∑ markdown, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.`;

export async function semanticChunking(text: string): Promise<SemanticChunk[]> {
    // For very long texts, split into manageable parts first
    const MAX_INPUT_LENGTH = 8000; // ~2K tokens (safer for 4k-8k output)

    if (text.length > MAX_INPUT_LENGTH) {
        // Split long text into parts and process each
        const parts: string[] = [];
        let start = 0;

        while (start < text.length) {
            // Find a good breaking point (paragraph or sentence)
            let end = Math.min(start + MAX_INPUT_LENGTH, text.length);

            if (end < text.length) {
                // Try to find paragraph break
                const paragraphBreak = text.lastIndexOf('\n\n', end);
                if (paragraphBreak > start + MAX_INPUT_LENGTH / 2) {
                    end = paragraphBreak + 2;
                } else {
                    // Try sentence break
                    const sentenceBreak = text.lastIndexOf('. ', end);
                    if (sentenceBreak > start + MAX_INPUT_LENGTH / 2) {
                        end = sentenceBreak + 2;
                    }
                }
            }

            parts.push(text.slice(start, end));
            start = end;
        }

        // Process each part
        const allChunks: SemanticChunk[] = [];
        let globalIndex = 0;

        for (const part of parts) {
            const partChunks = await processTextPart(part);
            for (const chunk of partChunks) {
                allChunks.push({
                    ...chunk,
                    index: globalIndex++,
                });
            }
        }

        return allChunks;
    }

    return processTextPart(text);
}

async function processTextPart(text: string): Promise<SemanticChunk[]> {
    try {
        const response = await litellmService.chatCompletion({
            messages: [
                { role: 'system', content: SEMANTIC_CHUNKING_PROMPT },
                { role: 'user', content: text },
            ],
            temperature: 0.3,
            max_tokens: 8192,
        });

        const content = response.choices[0]?.message?.content;

        if (!content) {
            throw new Error('Empty response from LLM');
        }

        console.log('ü§ñ Raw LLM Chunking Response:', content);

        // Clean content from markdown code blocks
        let cleanContent = content.replace(/```json/g, '').replace(/```/g, '').trim();

        // Try to parse JSON from response
        const jsonMatch = cleanContent.match(/\{[\s\S]*\}|\[[\s\S]*\]/);

        if (!jsonMatch) {
            // Try to find if it's just an array
            throw new Error('No JSON found in response');
        }

        let parsed;
        try {
            parsed = JSON.parse(jsonMatch[0]);
        } catch (e) {
            console.error('Failed to parse JSON:', jsonMatch[0]);
            throw new Error('Invalid JSON format');
        }

        // Handle both {chunks: [...]} and [...] formats
        const chunksArray = Array.isArray(parsed) ? parsed : parsed.chunks;

        if (!Array.isArray(chunksArray)) {
            throw new Error('Invalid response format: chunks array not found');
        }

        return chunksArray.map((chunk: any, i: number) => ({
            index: chunk.index ?? i,
            title: chunk.title || `Chunk ${i}`,
            text: chunk.text || '',
        }));

    } catch (error: any) {
        console.error('Semantic chunking error:', error.message);

        // Fallback: return text as single chunk
        return [{
            index: 0,
            title: 'Document',
            text: text,
        }];
    }
}
